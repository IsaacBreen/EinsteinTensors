{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'einstein_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dy/bfqxrmw10w160dt3drl9_0f80000gn/T/ipykernel_16310/1437724968.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meinstein_tensors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax_codegen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyperclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einstein_tensors'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random, vmap, jit, grad\n",
    "from einstein import jax_codegen\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Linear:\n",
      "    def __init__(self, o):\n",
      "        self.old_init = self.init\n",
      "        self.init = lambda *args, **kwargs: self.old_init(*args, o=o, **kwargs)\n",
      "\n",
      "    @staticmethod\n",
      "    def init(key, i, o, **kwargs):\n",
      "        keys = jax.random.split(key, 2)\n",
      "        return {\n",
      "            \"w_o_i\": jax.random.normal(keys[0], shape=[o, i]),\n",
      "            \"b_o\":   jax.random.normal(keys[1], shape=[o])\n",
      "        }\n",
      "    \n",
      "    @staticmethod\n",
      "    @jit\n",
      "    @lambda apply: vmap(apply, in_axes=(0, None))\n",
      "    def apply(x_i, params):\n",
      "        o, i = params['w_o_i'].shape[0], x_i.shape[0]\n",
      "        z_o = jnp.einsum('oi,i->o', params['w_o_i'], x_i) + params['b_o'] + i + o*i + (o)*jnp.einsum('i->', x_i) + (o)*jnp.einsum('i->', x_i)\n",
      "        return z_o\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"\n",
    "function Linear(x)\n",
    "    z[o] = w[o,i] * x[i] + b[o] + 1[o] + 1[i] + 1[o_1,i] + x[i]*1[o_2,i] + x[i]*1[o_3]\n",
    "    return z[o]\n",
    "end\n",
    "\"\"\"\n",
    "c = jax_codegen(s)\n",
    "print(c)\n",
    "pyperclip.copy(c)\n",
    "exec(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, o):\n",
    "        self.old_init = self.init\n",
    "        self.init = lambda *args, **kwargs: self.old_init(*args, o=o, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def init(key, i, o, **kwargs):\n",
    "        keys = jax.random.split(key, 2)\n",
    "        return {\n",
    "            \"w_o_i\": jax.random.normal(keys[0], shape=[o, i]),\n",
    "            \"b_o\":   jax.random.normal(keys[1], shape=[o])\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit\n",
    "    @lambda apply: vmap(apply, in_axes=(0, None))\n",
    "    def apply(x_i, params):\n",
    "        o, i = params['w_o_i'].shape[0], x_i.shape[0]\n",
    "        z_o = jnp.einsum('oi,i->o', params['w_o_i'], x_i) + params['b_o'] + i + o*i + (o)*jnp.einsum('i->', x_i) + (o)*jnp.einsum('i->', x_i)\n",
    "        return z_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1.shape: (1, 200)\n",
      "out2.shape: (1, 200)\n",
      "Trials 1: generated vs hand-written\n",
      "10.7 µs ± 2.42 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "9.86 µs ± 794 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Trials 2: hand-written vs generated\n",
      "9.81 µs ± 1.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "10.3 µs ± 718 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "jnp.isclose(out1, out2).mean(): 1.0\n",
      "jnp.abs(lin.apply(x, params) - linear(x, params['w_o_i'], params['b_o'])).mean(): 0.0\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def linear(x, W, b):\n",
    "    return jnp.dot(W, x) + b + 500 + 500*200 + 200*x.sum() + 200*x.sum()\n",
    "\n",
    "linear = jit(vmap(linear, in_axes=(0, None, None)))\n",
    "\n",
    "lin = Linear(o=200)\n",
    "params = lin.init(jax.random.PRNGKey(0), i=500)\n",
    "# print(params)\n",
    "x = random.normal(jax.random.PRNGKey(0), shape=[1,500])\n",
    "\n",
    "out1 = lin.apply(x, params).block_until_ready()\n",
    "out2 = linear(x, params['w_o_i'], params['b_o']).block_until_ready()\n",
    "\n",
    "print(f\"out1.shape: {out1.shape}\")\n",
    "print(f\"out2.shape: {out2.shape}\")\n",
    "\n",
    "print(\"Trials 1: generated vs hand-written\")\n",
    "%timeit -n 100 out1 = lin.apply(x, params).block_until_ready()\n",
    "%timeit -n 100 out2 = linear(x, params['w_o_i'], params['b_o']).block_until_ready()\n",
    "print(\"Trials 2: hand-written vs generated\")\n",
    "%timeit -n 100 out2 = linear(x, params['w_o_i'], params['b_o']).block_until_ready()\n",
    "%timeit -n 100 out1 = lin.apply(x, params).block_until_ready()\n",
    "\n",
    "\n",
    "print(f\"jnp.isclose(out1, out2).mean(): {jnp.isclose(out1, out2).mean()}\")\n",
    "print(f\"jnp.abs(lin.apply(x, params) - linear(x, params['w_o_i'], params['b_o'])).mean(): {jnp.abs(lin.apply(x, params) - linear(x, params['w_o_i'], params['b_o'])).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out2.shape: (1,)\n",
      "out3.shape: (1,)\n",
      "Trials 1: generated vs hand-written\n",
      "16.6 µs ± 862 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "83.1 µs ± 33.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Trials 2: hand-written vs generated\n",
      "68.3 µs ± 682 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "16.8 µs ± 637 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Trials 3: hand-written vs generated\n",
      "16.5 µs ± 536 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "68.6 µs ± 1.23 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "jnp.isclose(out2, out3).mean(): 1.0\n"
     ]
    }
   ],
   "source": [
    "class Linear2:\n",
    "    def __init__(self, o):\n",
    "        self.old_init = self.init\n",
    "        self.init = lambda *args, **kwargs: self.old_init(*args, o=o, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def init(key, i, o, **kwargs):\n",
    "        keys = jax.random.split(key, 2)\n",
    "        return {\n",
    "            \"w_o_i\": jax.random.normal(keys[0], shape=[o, i]),\n",
    "            \"b_o\":   jax.random.normal(keys[1], shape=[o])\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit\n",
    "    @lambda apply: vmap(apply, in_axes=(0, None))\n",
    "    def apply(x_i, params):\n",
    "        z_o = x_i.sum() + x_i.shape[0]\n",
    "        return z_o\n",
    "\n",
    "class Linear3:\n",
    "    def __init__(self, o):\n",
    "        self.old_init = self.init\n",
    "        self.init = lambda *args, **kwargs: self.old_init(*args, o=o, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def init(key, i, o, **kwargs):\n",
    "        keys = jax.random.split(key, 2)\n",
    "        return {\n",
    "            \"w_o_i\": jax.random.normal(keys[0], shape=[o, i]),\n",
    "            \"b_o\":   jax.random.normal(keys[1], shape=[o])\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit\n",
    "    @lambda apply: vmap(apply, in_axes=(0, None))\n",
    "    def apply(x_i, params):\n",
    "        z_o = jnp.einsum('i,i', x_i, jnp.ones(x_i.shape[0]),)\n",
    "        return z_o\n",
    "\n",
    "input_size = 50000\n",
    "key = jax.random.PRNGKey(0)\n",
    "x = random.normal(key, shape=[1,input_size])\n",
    "lin2 = Linear2(o=200)\n",
    "params2 = lin2.init(jax.random.PRNGKey(0), i=input_size)\n",
    "lin3 = Linear3(o=200)\n",
    "params3 = lin3.init(jax.random.PRNGKey(0), i=input_size)\n",
    "\n",
    "out2 = lin2.apply(x, params2).block_until_ready()\n",
    "out3 = lin2.apply(x, params3).block_until_ready()\n",
    "print(f\"out2.shape: {out2.shape}\")\n",
    "print(f\"out3.shape: {out3.shape}\")\n",
    "\n",
    "print(\"Trials 1: generated vs hand-written\")\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "x = random.normal(subkey, shape=[1,input_size])\n",
    "%timeit -n 100 out2 = lin2.apply(x, params2).block_until_ready()\n",
    "%timeit -n 100 out3 = lin3.apply(x, params3).block_until_ready()\n",
    "print(\"Trials 2: hand-written vs generated\")\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "x = random.normal(subkey, shape=[1,input_size])\n",
    "%timeit -n 100 out3 = lin3.apply(x, params2).block_until_ready()\n",
    "%timeit -n 100 out2 = lin2.apply(x, params3).block_until_ready()\n",
    "print(\"Trials 3: hand-written vs generated\")\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "x = random.normal(subkey, shape=[1,input_size])\n",
    "%timeit -n 100 out2 = lin2.apply(x, params2).block_until_ready()\n",
    "%timeit -n 100 out3 = lin3.apply(x, params3).block_until_ready()\n",
    "\n",
    "\n",
    "print(f\"jnp.isclose(out2, out3).mean(): {jnp.isclose(out2, out3).mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# s = \"\"\"\n",
    "# function MultiheadSelfAttention(x)\n",
    "#     x[t,i] = x[t,i] * gx[i] * (x[t,i_1]^2 * 1[i_1] / (x[t,i_2] * 1[i_2]))^0.5\n",
    "#     q[h,t,j] = q[h,j,i] * x[t,i]\n",
    "#     k[h,t,j] = k[h,j,i] * x[t,i]\n",
    "#     v[h,t,j] = v[h,j,i] * x[t,i]\n",
    "#     a[h,t_1,t_2] = jnp.exp(q[h,t_1,j] * k[h,t_2,j])\n",
    "#     u[t,k] = activation(wu[h,j,k] * a[h,t,t_2] * v[h,t_2,j] + bu[k])\n",
    "#     z[t,i] = wz[i,k] * u[t,k] + bz[i]\n",
    "#     z[t,i] = z[t,i] * gz[i] * (z[t,i]^2 * 1[i] / (z[t,i] + 1[i]))\n",
    "#     return z[t,i]\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_jax_module(\"\"\"\n",
    "function MultiheadSelfAttention(x)\n",
    "    x[t,i] = x[t,i] * gx[i]\n",
    "    q[h,t,j] = q[h,j,i] * x[t,i]\n",
    "    k[h,t,j] = k[h,j,i] * x[t,i]\n",
    "    v[h,t,j] = v[h,j,i] * x[t,i]\n",
    "    a[h,t_1,t_2] = jnp.exp(q[h,t_1,j] * k[h,t_2,j])\n",
    "    u[t,k] = activation(wu[h,j,k] * a[h,t,t_2] * v[h,t_2,j] + bu[k])\n",
    "    z[t,i] = wz[t,k] * u[t,k] + bz[i]\n",
    "    return z[t,i]\n",
    "end\n",
    "\"\"\")\n",
    "print(jax_codegen(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    # Squared activation function\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-2.3988705e+02 -2.3823891e+02]\n",
      "  [-1.1472459e+07 -1.1472458e+07]]]\n"
     ]
    }
   ],
   "source": [
    "class MultiheadSelfAttention:\n",
    "    def __init__(self, k, j, h):\n",
    "        self.old_init = self.init\n",
    "        self.init = lambda *args, **kwargs: self.old_init(*args, k=k, j=j, h=h, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def init(key, i, t, k, j, h, **kwargs):\n",
    "        keys = jax.random.split(key, 8)\n",
    "        return {\n",
    "            \"gx_i\": jax.random.normal(keys[0], shape=[i]),\n",
    "            \"q_h_j_i\": jax.random.normal(keys[1], shape=[h, j, i]),\n",
    "            \"k_h_j_i\": jax.random.normal(keys[2], shape=[h, j, i]),\n",
    "            \"v_h_j_i\": jax.random.normal(keys[3], shape=[h, j, i]),\n",
    "            \"wu_h_j_k\": jax.random.normal(keys[4], shape=[h, j, k]),\n",
    "            \"bu_k\": jax.random.normal(keys[5], shape=[k]),\n",
    "            \"wz_t_k\": jax.random.normal(keys[6], shape=[t, k]),\n",
    "            \"bz_i\": jax.random.normal(keys[7], shape=[i])\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    @lambda apply: vmap(apply, in_axes=(0, None))\n",
    "    def apply(x_t_i, params):\n",
    "        x_t_i = ((x_t_i*params['gx_i'][None, :])).sum(axis=[])\n",
    "        q_h_t_j = ((params['q_h_j_i'][:, None, :, :]*x_t_i[None, :, None, :])).sum(axis=[3])\n",
    "        k_h_t_j = ((params['k_h_j_i'][:, None, :, :]*x_t_i[None, :, None, :])).sum(axis=[3])\n",
    "        v_h_t_j = ((params['v_h_j_i'][:, None, :, :]*x_t_i[None, :, None, :])).sum(axis=[3])\n",
    "        a_h_t_t = (jnp.exp((q_h_t_j[:, :, None, :]*k_h_t_j[:, None, :, :]))).sum(axis=[3])\n",
    "        u_t_k = (activation(((params['wu_h_j_k'].transpose((2, 0, 1))[None, :, :, :, None]*a_h_t_t.transpose((1, 0, 2))[:, None, :, None, :]*v_h_t_j.transpose((0, 2, 1))[None, None, :, :, :]).sum(axis=[2, 3, 4], keepdims=True)+(params['bu_k'][None, :, None, None, None]).sum(axis=[2, 3, 4], keepdims=True)))).sum(axis=[2, 3, 4])\n",
    "        z_t_i = (((params['wz_t_k'][:, None, :]*u_t_k[:, None, :]).sum(axis=[2], keepdims=True)+(params['bz_i'][None, :, None]).sum(axis=[2], keepdims=True))).sum(axis=[2])\n",
    "        return z_t_i\n",
    "\n",
    "\n",
    "attention = MultiheadSelfAttention(k=2, j=2, h=2)\n",
    "params = attention.init(jax.random.PRNGKey(0), i=2, t=2)\n",
    "# print(params)\n",
    "x = jnp.array([[[0,1], [2,3]]])\n",
    "out = attention.apply(x, params)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4e86c934b375bdfbcaa1924885d620a34ad2919c9a02314b4bebd1564549087"
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
