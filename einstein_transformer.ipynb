{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaacbreen/miniforge3/envs/py39/lib/python3.9/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random, vmap, jit, grad\n",
    "from einstein import jax_codegen, build_jax_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Linear:\n",
      "    def __init__(self, o):\n",
      "        self.old_init = self.init\n",
      "        self.init = lambda *args, **kwargs: self.old_init(*args, o=o, **kwargs)\n",
      "\n",
      "    @staticmethod\n",
      "    def init(key, i, o, **kwargs):\n",
      "        keys = jax.random.split(key, 2)\n",
      "        return {\n",
      "            \"w_o_i\": jax.random.normal(keys[0], shape=[o, i]),\n",
      "            \"b_o\":   jax.random.normal(keys[1], shape=[o])\n",
      "        }\n",
      "    \n",
      "    @staticmethod\n",
      "    @jit\n",
      "    @lambda apply: vmap(apply, in_axes=(0, None))\n",
      "    def apply(x_i, params):\n",
      "        z_o = (jnp.einsum('oi,i->o', params['w_o_i'], x_i)) + params['b_o']\n",
      "        return z_o\n"
     ]
    }
   ],
   "source": [
    "einstein_code = \"\"\"\n",
    "function Linear(x)\n",
    "    z[o] = w[o,i] * x[i] + b[o]\n",
    "    return z[o]\n",
    "end\n",
    "\"\"\"\n",
    "python_code = jax_codegen(einstein_code)\n",
    "print(python_code)\n",
    "exec(python_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class MultiheadSelfAttention:\n",
      "    def __init__(self, k, h, j):\n",
      "        self.old_init = self.init\n",
      "        self.init = lambda *args, **kwargs: self.old_init(*args, k=k, h=h, j=j, **kwargs)\n",
      "\n",
      "    @staticmethod\n",
      "    def init(key, t, i, k, h, j, **kwargs):\n",
      "        keys = jax.random.split(key, 9)\n",
      "        return {\n",
      "            \"gx_i\":     jax.random.normal(keys[0], shape=[i]),\n",
      "            \"q_h_j_i\":  jax.random.normal(keys[1], shape=[h, j, i]),\n",
      "            \"k_h_j_i\":  jax.random.normal(keys[2], shape=[h, j, i]),\n",
      "            \"v_h_j_i\":  jax.random.normal(keys[3], shape=[h, j, i]),\n",
      "            \"wu_h_j_k\": jax.random.normal(keys[4], shape=[h, j, k]),\n",
      "            \"bu_k\":     jax.random.normal(keys[5], shape=[k]),\n",
      "            \"wz_i_k\":   jax.random.normal(keys[6], shape=[i, k]),\n",
      "            \"bz_i\":     jax.random.normal(keys[7], shape=[i]),\n",
      "            \"gz_i\":     jax.random.normal(keys[8], shape=[i])\n",
      "        }\n",
      "    \n",
      "    @staticmethod\n",
      "    @jit\n",
      "    @lambda apply: vmap(apply, in_axes=(0, None))\n",
      "    def apply(x_t_i, params):\n",
      "        x_t_i = jnp.einsum('ti,i,t->ti', x_t_i, params['gx_i'], (jnp.einsum('ti,t->t', x_t_i**2, (jnp.einsum('ti->t', x_t_i))**-1))**0.5)\n",
      "        q_h_t_j = jnp.einsum('hji,ti->htj', params['q_h_j_i'], x_t_i)\n",
      "        k_h_t_j = jnp.einsum('hji,ti->htj', params['k_h_j_i'], x_t_i)\n",
      "        v_h_t_j = jnp.einsum('hji,ti->htj', params['v_h_j_i'], x_t_i)\n",
      "        a_h_t_t = jnp.exp(jnp.einsum('htj,haj->hta', q_h_t_j, k_h_t_j))\n",
      "        u_t_k = activation((jnp.einsum('hjk,hat,htj->ak', params['wu_h_j_k'], a_h_t_t, v_h_t_j)) + params['bu_k'][None, :])\n",
      "        z_t_i = (jnp.einsum('ik,tk->ti', params['wz_i_k'], u_t_k)) + params['bz_i'][None, :]\n",
      "        z_t_i = jnp.einsum('ti,i,ti->ti', z_t_i, params['gz_i'], jnp.einsum('ti,ti->ti', z_t_i**2, (z_t_i)**-1))\n",
      "        return z_t_i\n"
     ]
    }
   ],
   "source": [
    "def activation(x):\n",
    "    return x**2\n",
    "    \n",
    "einstein_code = \"\"\"\n",
    "function MultiheadSelfAttention(x)\n",
    "    x[t,i] = x[t,i] * gx[i] * (x[t,i_1]^2 * 1[i_1] / (x[t,i_2] * 1[i_2]))^0.5\n",
    "    q[h,t,j] = q[h,j,i] * x[t,i]\n",
    "    k[h,t,j] = k[h,j,i] * x[t,i]\n",
    "    v[h,t,j] = v[h,j,i] * x[t,i]\n",
    "    a[h,t_1,t_2] = jnp.exp(q[h,t_1,j] * k[h,t_2,j])\n",
    "    u[t,k] = activation(wu[h,j,k] * a[h,t,t_2] * v[h,t_2,j] + bu[k])\n",
    "    z[t,i] = wz[i,k] * u[t,k] + bz[i]\n",
    "    z[t,i] = z[t,i] * gz[i] * (z[t,i]^2 * 1[i] / (z[t,i] + 1[i]))\n",
    "    return z[t,i]\n",
    "end\n",
    "\"\"\"\n",
    "python_code = jax_codegen(einstein_code)\n",
    "print(python_code)\n",
    "exec(python_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can copy and paste the output code above into a new cell (as below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttention:\n",
    "    def __init__(self, j, k, h):\n",
    "        self.old_init = self.init\n",
    "        self.init = lambda *args, **kwargs: self.old_init(*args, j=j, k=k, h=h, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def init(key, t, i, j, k, h, **kwargs):\n",
    "        keys = jax.random.split(key, 9)\n",
    "        return {\n",
    "            \"gx_i\":     jax.random.normal(keys[0], shape=[i]),\n",
    "            \"q_h_j_i\":  jax.random.normal(keys[1], shape=[h, j, i]),\n",
    "            \"k_h_j_i\":  jax.random.normal(keys[2], shape=[h, j, i]),\n",
    "            \"v_h_j_i\":  jax.random.normal(keys[3], shape=[h, j, i]),\n",
    "            \"wu_h_j_k\": jax.random.normal(keys[4], shape=[h, j, k]),\n",
    "            \"bu_k\":     jax.random.normal(keys[5], shape=[k]),\n",
    "            \"wz_i_k\":   jax.random.normal(keys[6], shape=[i, k]),\n",
    "            \"bz_i\":     jax.random.normal(keys[7], shape=[i]),\n",
    "            \"gz_i\":     jax.random.normal(keys[8], shape=[i])\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit\n",
    "    @lambda apply: vmap(apply, in_axes=(0, None))\n",
    "    def apply(x_t_i, params):\n",
    "        x_t_i = jnp.einsum('ti,i,t->ti', x_t_i, params['gx_i'], (jnp.einsum('ti,t->t', x_t_i**2, (jnp.einsum('ti->t', x_t_i))**-1))**0.5)\n",
    "        q_h_t_j = jnp.einsum('hji,ti->htj', params['q_h_j_i'], x_t_i)\n",
    "        k_h_t_j = jnp.einsum('hji,ti->htj', params['k_h_j_i'], x_t_i)\n",
    "        v_h_t_j = jnp.einsum('hji,ti->htj', params['v_h_j_i'], x_t_i)\n",
    "        a_h_t_t = jnp.exp(jnp.einsum('htj,haj->hta', q_h_t_j, k_h_t_j))\n",
    "        u_t_k = activation((jnp.einsum('hjk,hat,htj->ak', params['wu_h_j_k'], a_h_t_t, v_h_t_j)) + params['bu_k'][None, :])\n",
    "        z_t_i = (jnp.einsum('ik,tk->ti', params['wz_i_k'], u_t_k)) + params['bz_i'][None, :]\n",
    "        z_t_i = jnp.einsum('ti,i,ti->ti', z_t_i, params['gz_i'], jnp.einsum('ti,ti->ti', z_t_i**2, (z_t_i)**-1))\n",
    "        return z_t_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[54.03943   10.082219  88.0705    10.502079  13.076569  -9.914572\n",
      "   -2.1415646]\n",
      "  [54.03943   10.082219  88.0705    10.502079  13.076569  -9.914572\n",
      "   -2.1415646]\n",
      "  [54.03943   10.082219  88.0705    10.502079  13.076569  -9.914572\n",
      "   -2.1415646]]]\n"
     ]
    }
   ],
   "source": [
    "attention = MultiheadSelfAttention(h=2,k=3,j=5)\n",
    "params = attention.init(key=random.PRNGKey(0), t=None, i=7)\n",
    "x = jnp.ones((1,3,7)) # dummy data\n",
    "y = attention.apply(x, params)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einstein_code = \"\"\"\n",
    "function Linear(x)\n",
    "    z[o] = w[o,i] * x[i] + b[o]\n",
    "    return z[o]\n",
    "end\n",
    "\n",
    "function TwoLayerLinear(x)\n",
    "    y[j] = Linear(x[i])\n",
    "    z[o] = Linear(y[j])\n",
    "    return z[o]\n",
    "end\n",
    "\n",
    "function MLP(x)\n",
    "    y[0,j] = activation(w[0,j,i] * x[i] + b[0,j])\n",
    "    y[l,j] = activation(w[l,j,i] * y[l-1,j] + b[l,j])\n",
    "    z[o] = w[o,k,j] * y[l,j] + b[o,k]\n",
    "end\n",
    "\n",
    "function Conv1D(x)\n",
    "    y[\n",
    "end\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4e86c934b375bdfbcaa1924885d620a34ad2919c9a02314b4bebd1564549087"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
